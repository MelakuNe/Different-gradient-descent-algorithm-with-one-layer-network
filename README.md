## Optimizing a perceptron for document classification https://www.kaggle.com/competitions/intro-to-stoch-opti-project-1/overview
Objectives of the project and evaluation
## Objective 1
The first goal of this project is to build a classifier for documents using a neural network with one layer a.k.a. a perceptron, which is a classifier based on a non-linear activation applied to a linear model.

Important rule: You need to program everything yourself, no use of libraries of machine learning framework such as PyTorch, Tensorflow, etc.
The main idea is simple: go to low programming level for machine learning, implement the methods to understand in more details thanks to deep practice.

## Objective 2
The second goal is to compare at least three different stochastic methods (seen during the lectures) to achieve the first goal, for instance : standard gradient descent (GD), stochastic gradient descent (SGD), MiniBatch SGD, SGD with momentum, SAGA, etc
You should discuss the effectiveness of these methods:

* which one would you recommend in which situation?
* how to select the step size, which rule did you use for getting your best results, what is your general feedback ?
